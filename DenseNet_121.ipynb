{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "V28"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#**<font color='yellow'>Model Name: DenseNet-121</font>**\n",
        "##**Date: 21<sup>st</sup>January 2020**"
      ],
      "metadata": {
        "id": "3c6MxojNCeqF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Mount Google Drive**"
      ],
      "metadata": {
        "id": "H4rfZWB1CuzK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Rrz_5Y5AM_v"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####**Import Necessary Library**"
      ],
      "metadata": {
        "id": "M4KXXe-BC1EY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "import glob as gb\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "kQMrCRzPC4V5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
        "from tensorflow.keras.layers.experimental.preprocessing import RandomFlip, RandomRotation\n",
        "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
        "from tensorflow.keras.layers import Conv2D, MaxPool2D ,LeakyReLU"
      ],
      "metadata": {
        "id": "06Mq3xHyHBjZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####**Define directories path**"
      ],
      "metadata": {
        "id": "ltxTsgMlr1q_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Define the directories for training, testing, and validation\n",
        "train_directory = 'path to the directory'\n",
        "test_directory = 'path to the directory'\n",
        "valid_directory = 'path to the directory'"
      ],
      "metadata": {
        "id": "j5iYJBbXLam1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####**Create dataset function using keras/Tensorflow**"
      ],
      "metadata": {
        "id": "99Pc8KxS6L6V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "IMG_SIZE = (299, 299)  # define resolution (299,299) /(224,224)\n",
        "BATCH_SIZE = 128       # varies from dataset to datset prefferable 128/68/32"
      ],
      "metadata": {
        "id": "bcSESb1NFk0x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create TensorFlow datasets for training, testing, and validation\n",
        "#you can customize parameters as per dataset\n",
        "train_dataset = image_dataset_from_directory(\n",
        "    train_directory,\n",
        "    shuffle=True,\n",
        "    labels='inferred',\n",
        "    batch_size=BATCH_SIZE,\n",
        "    image_size=IMG_SIZE,\n",
        "    color_mode='rgb',\n",
        "    seed=42\n",
        ")\n",
        "\n",
        "test_dataset = image_dataset_from_directory(\n",
        "    test_directory,\n",
        "    shuffle=True,\n",
        "    labels='inferred',\n",
        "    batch_size=BATCH_SIZE,\n",
        "    image_size=IMG_SIZE,\n",
        "    color_mode='rgb',\n",
        "    seed=42\n",
        ")\n",
        "\n",
        "valid_dataset = image_dataset_from_directory(\n",
        "    valid_directory,\n",
        "    shuffle=True,\n",
        "    labels='inferred',\n",
        "    batch_size=BATCH_SIZE,\n",
        "    image_size=IMG_SIZE,\n",
        "    color_mode='rgb',\n",
        "    seed=42\n",
        ")"
      ],
      "metadata": {
        "id": "6Q5mCJtiF5_p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Optional: You can also specify the class names if you have a specific order for your classes\n",
        "class_names = train_dataset.class_names\n",
        "\n",
        "# Print class names\n",
        "print(\"Class Names:\", class_names)"
      ],
      "metadata": {
        "id": "QwhgT6uBGVP4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#plot bar chart for demonstrating data size\n",
        "sns.set_theme(style=\"whitegrid\")\n",
        "Dataset = []\n",
        "for folder in os.listdir(train_directory):\n",
        "    files = gb.glob(pathname=str(train_directory + \"/\" + folder +\"/*.*\"))\n",
        "    Dataset.append(len(files))\n",
        "plt.figure(figsize=(13,7))                    ## change the len size\n",
        "sns.barplot(x=[ \"0\",\"1\"], y=Dataset, palette=\"rocket\")  ## change the class name\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ij6ZO6QQGa0B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <font color='blue'>Build the Model</font>"
      ],
      "metadata": {
        "id": "n_GMjBaXOpD6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "base_learning_rate = 0.001 # prefferable lr is 0.0001 or 0.001\n",
        "IMG_SHAPE = IMG_SIZE +(3,)"
      ],
      "metadata": {
        "id": "-OalbxVyBbHA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## N.B:download the weights (imagenet/...) and model (Resnet50/ResNet100/InceptionV3...)\n",
        "base_model = tf.keras.applications.DenseNet121(input_shape=IMG_SHAPE, include_top= False, weights='imagenet')"
      ],
      "metadata": {
        "id": "tl9Pm1lQO6Up"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(base_model)\n",
        "## N.B: changle the model name\n",
        "preprocess_input = tf.keras.applications.densenet.preprocess_input"
      ],
      "metadata": {
        "id": "wJWvn2iEO92h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nb_layers = len(base_model.layers)\n",
        "print(\"Numbers of Layers =\" , nb_layers)\n",
        "print(base_model.layers[nb_layers - 2].name)  # pre- Last name\n",
        "print(base_model.layers[nb_layers - 1].name)"
      ],
      "metadata": {
        "id": "hHByKSYiPAfk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# iterate over first batch (32 image) in trainset\n",
        "image_batch, label_batch = next(iter(train_dataset))  # 32 image arrays\n",
        "feature_batch = base_model(image_batch)  # run the model on those 32 image (base model with its 1000 causes classification)\n",
        "print(feature_batch.shape)  # 32 for number of images in this batch and 1000 for classes"
      ],
      "metadata": {
        "id": "02wLshtjPBxI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_model( image_shape=IMG_SHAPE):\n",
        "    ''' Define a tf.keras model for multi-class classification out of the *model name* (Resnet/Inception...) '''\n",
        "    ##change the *model name*\n",
        "    downloaded_model = tf.keras.applications.DenseNet121(input_shape=IMG_SHAPE, include_top= False, weights='imagenet')\n",
        "    downloaded_model.trainable = True\n",
        "    for layer in downloaded_model.layers[0 : 291]:\n",
        "        layer.trainable = False\n",
        "\n",
        "    inputs = tf.keras.Input(image_shape)\n",
        "    x = preprocess_input(inputs)\n",
        "    x = downloaded_model(x , training=False)\n",
        "    x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
        "    x = tf.keras.layers.Dropout(0.5)(x)\n",
        "    prediction_layer = tf.keras.layers.Dense(7 ,activation = \"softmax\")   ## change the first parameter according to the class len\n",
        "    outputs = prediction_layer(x)\n",
        "    model = tf.keras.Model(inputs, outputs)\n",
        "    model.summary()\n",
        "    return model"
      ],
      "metadata": {
        "id": "oIUFhaRnPgYj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## specify function name as model name\n",
        "model = create_model(IMG_SHAPE)"
      ],
      "metadata": {
        "id": "IA6HsX4xY_k1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## customize optimizer as Nadam or Adam\n",
        "model.compile(optimizer=tf.keras.optimizers.Nadam(learning_rate=base_learning_rate),\n",
        "                           loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
        "                           metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "5snFX_C-cbRe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify the full path where you want to create the folder\n",
        "folder_path = \"/path/to/your/directory/new_folder\"\n",
        "\n",
        "# Create the folder\n",
        "os.makedirs(folder_path, exist_ok=True)"
      ],
      "metadata": {
        "id": "z4IwPKmODIfy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "## set the path name as *dataset/Lr/optimizer_name/model_name*\n",
        "model_filepath=\"/content/drive/MyDrive/dataset/Lr/optimizer_name/model_name-{epoch:02d}-{val_accuracy:.4f}.hdf5\"\n",
        "checkpoint = ModelCheckpoint(\n",
        "    filepath = model_filepath ,\n",
        "    monitor ='val_accuracy',\n",
        "    mode = 'max' ,\n",
        "    save_best_only =True ,\n",
        "    verbose = 1\n",
        ")"
      ],
      "metadata": {
        "id": "EebFVeeCc6et"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#avoid random weight initialization do automatic if you can\n",
        "\n",
        "total = 0\n",
        "for i in range(0,len(Dataset )) :\n",
        "    total +=Dataset [i]\n",
        "\n",
        "weight_for_0 = (1 / Dataset [0]) * (total / 2.0)\n",
        "weight_for_1 = (1 / Dataset [1]) * (total / 2.0)\n",
        "\n",
        "\n",
        "class_weight = {0: weight_for_0, 1: weight_for_1}\n",
        "\n",
        "print('Weight for class 0: {:.2f}'.format(weight_for_0))\n",
        "print('Weight for class 1: {:.2f}'.format(weight_for_1))"
      ],
      "metadata": {
        "id": "MXhh4s7xDk3x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####**Training model**"
      ],
      "metadata": {
        "id": "MQ70xk06BAhR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## change hyperparameter such as epoches\n",
        "history = model.fit(train_dataset , verbose=2 , epochs=50 , class_weight=class_weight ,\n",
        "                               validation_data=valid_dataset , use_multiprocessing= True, callbacks =[checkpoint])"
      ],
      "metadata": {
        "id": "ASDsHJqQdXk2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Model evaluating on Test dataset**"
      ],
      "metadata": {
        "id": "3v7siWwLvSG3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.evaluate(test_dataset , verbose = 1)"
      ],
      "metadata": {
        "id": "iDopjvLRdc0_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(20, 15))\n",
        "for images, labels in test_dataset.take(1):\n",
        "    for i in range(25):\n",
        "        plt.subplot(5,5,i+1)\n",
        "        plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
        "        plt.axis(\"off\")\n",
        "        im2 = images[i].numpy().astype(\"uint8\")\n",
        "        img2 = tf.expand_dims(im2, 0)\n",
        "        predict = model.predict(img2)\n",
        "        predicted= class_names[np.argmax(predict)]\n",
        "        actual = class_names [labels[i].numpy().astype(\"uint8\")]\n",
        "        if (actual == predicted):\n",
        "            plt.title(predicted, fontsize=10, color= 'blue', pad=15);\n",
        "        else :\n",
        "            plt.title(actual, fontsize=10, color= 'red' ,pad=15);\n",
        "        plt.subplots_adjust(left=0.1,bottom=0.1, right=0.9,\n",
        "                            top=0.9, wspace=0.4,hspace=0.4)"
      ],
      "metadata": {
        "id": "Q6tp2J0udexP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**<font color='green'>Training and Validation Accuracy </font> & <font color='red'>Training and Validation Loss </font>**"
      ],
      "metadata": {
        "id": "eiTyZRgCpx2b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "acc = [0.] + history.history['accuracy']\n",
        "val_acc = [0.] + history.history['val_accuracy']\n",
        "\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "plt.figure(figsize=(17, 12))\n",
        "plt.subplot(2, 2, 1)\n",
        "plt.plot(acc, label='Training Accuracy')\n",
        "plt.plot(val_acc, label='Validation Accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.ylim([min(plt.ylim()),1])\n",
        "plt.title('Training and Validation Accuracy')\n",
        "\n",
        "plt.subplot(2, 2, 2)\n",
        "plt.plot(loss, label='Training Loss')\n",
        "plt.plot(val_loss, label='Validation Loss')\n",
        "plt.legend(loc='upper right')\n",
        "plt.ylabel('Cross Entropy')\n",
        "plt.ylim([0,3.0])\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "cffTY7xIdglH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**True and Predicted labels**"
      ],
      "metadata": {
        "id": "-IVeTL6Ks5Gm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import label_binarize\n",
        "\n",
        "# Initialize empty lists to store true labels and predicted labels\n",
        "true_labels = []\n",
        "predicted_labels = []\n",
        "\n",
        "# Iterate through the validation dataset and make predictions\n",
        "for images, labels in test_dataset:\n",
        "    predictions = model.predict(images)\n",
        "    predicted_labels.extend(np.argmax(predictions, axis=1))\n",
        "    true_labels.extend(labels.numpy())\n",
        "\n",
        "# Binarize the true and predicted labels\n",
        "true_labels_bin = label_binarize(true_labels, classes=np.unique(true_labels))\n",
        "predicted_labels_bin = label_binarize(predicted_labels, classes=np.unique(predicted_labels))"
      ],
      "metadata": {
        "id": "e9WMIn6qspog"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Classification Report**"
      ],
      "metadata": {
        "id": "TtyzzXJ4qarM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Generate the classification report\n",
        "report = classification_report(true_labels, predicted_labels, target_names=class_names)\n",
        "\n",
        "# Print the classification report\n",
        "print(report)"
      ],
      "metadata": {
        "id": "dS3ZX7Srqa4n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**ROC Curve for Multiclass Classification**"
      ],
      "metadata": {
        "id": "ybdZaU84rNYz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_auc_score, roc_curve, auc\n",
        "\n",
        "# Compute class-specific ROC AUC values\n",
        "roc_auc_per_class = []\n",
        "for i in range(len(class_names)):\n",
        "    roc_auc = roc_auc_score(true_labels_bin[:, i], predicted_labels_bin[:, i])\n",
        "    roc_auc_per_class.append(roc_auc)\n",
        "\n",
        "# Plot class-specific ROC curves\n",
        "plt.figure(figsize=(10, 8))\n",
        "\n",
        "for i in range(len(class_names)):\n",
        "    fpr, tpr, _ = roc_curve(true_labels_bin[:, i], predicted_labels_bin[:, i])\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "\n",
        "    plt.plot(fpr, tpr, label=f'ROC curve (area = {roc_auc:.2f}) for {class_names[i]}')\n",
        "\n",
        "plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "##change the model name\n",
        "plt.title('ROC Curves for Multiclass Classification of *model name*')\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()\n",
        "\n",
        "# Print class-specific ROC AUC values\n",
        "for i in range(len(class_names)):\n",
        "    print(f'ROC AUC for {class_names[i]}: {roc_auc_per_class[i]:.4f}')"
      ],
      "metadata": {
        "id": "81NLPeZSrNkw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Confusion Matrix**"
      ],
      "metadata": {
        "id": "DM_N8OgCpAGZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "conf_matrix = confusion_matrix(true_labels, predicted_labels)\n",
        "\n",
        "# Plot the confusion matrix as a heatmap\n",
        "plt.figure(figsize=(5, 3))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
        "plt.xlabel('Predicted Labels')\n",
        "plt.ylabel('True Labels')\n",
        "## change the model name\n",
        "plt.title('Confusion Matrix of *model_name*')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "J9gWVFubdk0Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Cohen's Kappa**"
      ],
      "metadata": {
        "id": "7akuXLlkpT3J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import cohen_kappa_score\n",
        "\n",
        "kappa = cohen_kappa_score(true_labels, predicted_labels)\n",
        "print(f'Cohen\\'s Kappa: {kappa:.4f}')"
      ],
      "metadata": {
        "id": "8ggV91WzpUCl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Matthews Correlation Coefficient (MCC)**"
      ],
      "metadata": {
        "id": "IABgqZUqpUUe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import matthews_corrcoef\n",
        "import numpy as np\n",
        "\n",
        "# Assuming true_labels and predicted_labels are multiclass labels\n",
        "mcc_values = [matthews_corrcoef(true_labels == i, predicted_labels == i) for i in np.unique(true_labels)]\n",
        "\n",
        "average_mcc = np.mean(mcc_values)\n",
        "print(f'Average Matthews Correlation Coefficient for Multiclass: {average_mcc:.4f}')"
      ],
      "metadata": {
        "id": "ujJsadsRpUq9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Right and Wrong Prediction**"
      ],
      "metadata": {
        "id": "LlJb4u7KDzLO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate total right and wrong predictions\n",
        "total_right = sum(1 for true, pred in zip(true_labels, predicted_labels) if true == pred)\n",
        "total_wrong = sum(1 for true, pred in zip(true_labels, predicted_labels) if true != pred)\n",
        "total_samples = len(true_labels)\n",
        "\n",
        "# Calculate wrong prediction percentage\n",
        "wrong_prediction_percentage = (total_wrong / total_samples) * 100\n",
        "\n",
        "print(\"Total Right Predictions:\", total_right)\n",
        "print(\"Total Wrong Predictions:\", total_wrong)\n",
        "print(\"Wrong Prediction Percentage: {:.2f}%\".format(wrong_prediction_percentage))\n"
      ],
      "metadata": {
        "id": "Dj7zAt2BDyqu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare data for plotting\n",
        "categories = ['Right Predictions', 'Wrong Predictions']\n",
        "values = [total_right, total_wrong]\n",
        "\n",
        "# Create a bar plot\n",
        "plt.figure(figsize=(6, 4))\n",
        "plt.bar(categories, values, color=['green', 'red'])\n",
        "plt.ylabel('Count')\n",
        "plt.title('Right and Wrong Predictions')\n",
        "\n",
        "# Show counts on top of the bars\n",
        "for i, v in enumerate(values):\n",
        "    plt.text(i, v + 0.1, str(v), ha='center', fontweight='bold')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "FGHE3bDMD4M0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate confusion matrix\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "TN, FP, FN, TP = cm.ravel()\n",
        "\n",
        "# Right and wrong classifications\n",
        "right_classifications = TP + TN\n",
        "wrong_classifications = FP + FN\n",
        "\n",
        "# Calculate percentages\n",
        "total = right_classifications + wrong_classifications\n",
        "right_percentage = (right_classifications / total) * 100\n",
        "wrong_percentage = (wrong_classifications / total) * 100\n",
        "\n",
        "# Data for the bar plot\n",
        "labels = ['Right', 'Wrong']\n",
        "percentages = [right_percentage, wrong_percentage]\n"
      ],
      "metadata": {
        "id": "7FswP0dKXToh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the bar plot\n",
        "plt.figure(figsize=(8, 5))\n",
        "bars = plt.bar(labels, percentages, color=['green', 'red'])\n",
        "\n",
        "# Add percentage labels above the bars\n",
        "for bar in bars:\n",
        "    yval = bar.get_height()\n",
        "    plt.text(bar.get_x() + bar.get_width()/2, yval, f'{yval:.1f}%',\n",
        "             ha='center', va='bottom')  # va: vertical alignment\n",
        "\n",
        "# Add titles and labels\n",
        "plt.title('Classification Results')\n",
        "plt.ylabel('Percentage (%)')\n",
        "plt.ylim(0, 100)  # Set y-axis limit to 100%\n",
        "plt.axhline(0, color='grey', linewidth=0.8, linestyle='--')  # Optional: Add a horizontal line at y=0\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "9aA_4KetXWZ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Misclassified Images**"
      ],
      "metadata": {
        "id": "TzcdChGoLr0W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import random\n",
        "\n",
        "# Function to load a specified number of random images from subfolders\n",
        "def load_random_images_from_subfolders(folder, num_images=2):\n",
        "    images = []\n",
        "    for subfolder in os.listdir(folder):\n",
        "        subfolder_path = os.path.join(folder, subfolder)\n",
        "        if os.path.isdir(subfolder_path):\n",
        "            image_files = os.listdir(subfolder_path)\n",
        "            # Randomly select images from this subfolder\n",
        "            selected_files = random.sample(image_files, min(num_images, len(image_files)))\n",
        "            for image_file in selected_files:\n",
        "                img_path = os.path.join(subfolder_path, image_file)\n",
        "                img = Image.open(img_path)\n",
        "                img = img.resize((64, 64))  # Resize to a consistent size if necessary\n",
        "                images.append(np.array(img) / 255.0)  # Normalize pixel values\n",
        "    return np.array(images)\n",
        "\n",
        "# Set the path to your test folder\n",
        "test_folder = 'path/to/test_folder/'  # Folder containing subfolders of images\n",
        "\n",
        "# Load a few random images from subfolders\n",
        "images = load_random_images_from_subfolders(test_folder, num_images=2)\n",
        "\n",
        "# Assuming you already have true and predicted labels loaded\n",
        "# Replace these with your actual arrays\n",
        "true_labels = np.array([0, 1])  # True labels for the loaded images\n",
        "predicted_labels = np.array([0, 1])  # Predicted labels for the loaded images\n",
        "\n",
        "# Set up the plot\n",
        "cols = 2  # Two columns: original and predicted\n",
        "rows = len(images)  # One row for each image\n",
        "\n",
        "fig, axes = plt.subplots(rows, cols, figsize=(10, 5 * rows))\n",
        "\n",
        "# Loop through and display images\n",
        "for i in range(len(images)):\n",
        "    original_image = images[i]\n",
        "    predicted_label = predicted_labels[i]\n",
        "    true_label = true_labels[i]\n",
        "\n",
        "    # Plot original image\n",
        "    axes[i, 0].imshow(original_image)\n",
        "    axes[i, 0].set_title(f'Original Label: {true_label}')\n",
        "    axes[i, 0].axis('off')\n",
        "\n",
        "    # Plot predicted image (optionally the same image)\n",
        "    axes[i, 1].imshow(original_image)\n",
        "    axes[i, 1].set_title(f'Predicted Label: {predicted_label}')\n",
        "    axes[i, 1].axis('off')\n",
        "\n",
        "# Adjust layout\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "1fb9_MwPLtPx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Sensitivity, Specificity**"
      ],
      "metadata": {
        "id": "QS32urpTE8ub"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the confusion matrix\n",
        "tn, fp, fn, tp = confusion_matrix(true_labels, predicted_labels).ravel()\n",
        "\n",
        "# Calculate sensitivity, specificity, and Jaccard index\n",
        "sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
        "specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
        "\n",
        "# Print the results\n",
        "print(f\"Sensitivity: {sensitivity:.2f}\")\n",
        "print(f\"Specificity: {specificity:.2f}\")"
      ],
      "metadata": {
        "id": "5jNSzz0nE8PB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Metrics for plotting\n",
        "metrics = [sensitivity, specificity]\n",
        "labels = ['Sensitivity', 'Specificity']\n",
        "\n",
        "# Create bar plot\n",
        "plt.bar(labels, metrics, color=['blue', 'orange'])\n",
        "plt.ylim(0, 1)  # Set y-axis limit from 0 to 1\n",
        "plt.ylabel('Score')\n",
        "plt.title('Sensitivity and Specificity')\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "e-PeL4MKKP3U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Jaccard index and the Dice score (or Dice coefficient)**"
      ],
      "metadata": {
        "id": "INAjezhOJn5x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the confusion matrix\n",
        "tn, fp, fn, tp = confusion_matrix(true_labels, predicted_labels).ravel()\n",
        "\n",
        "# Calculate Jaccard Index\n",
        "jaccard_index = tp / (tp + fp + fn) if (tp + fp + fn) > 0 else 0\n",
        "\n",
        "# Calculate Dice Score\n",
        "dice_score = (2 * tp) / (2 * tp + fp + fn) if (2 * tp + fp + fn) > 0 else 0\n",
        "\n",
        "# Print results\n",
        "print(f\"Jaccard Index: {jaccard_index:.2f}\")\n",
        "print(f\"Dice Score: {dice_score:.2f}\")"
      ],
      "metadata": {
        "id": "ZDKKwkceE8Ta"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Metrics for plotting\n",
        "metrics = [jaccard_index, dice_score]\n",
        "labels = ['Jaccard Index', 'Dice Score']\n",
        "\n",
        "# Create bar plot\n",
        "plt.bar(labels, metrics, color=['blue', 'orange'])\n",
        "plt.ylim(0, 1)  # Set y-axis limit from 0 to 1\n",
        "plt.ylabel('Score')\n",
        "plt.title('Performance Metrics')\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "i-fgPOLTJzAG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}