{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#**<font color='yellow'>Model Name: ResNet-50</font>**\n",
        "##**Date: 21<sup>st</sup>January 2020**"
      ],
      "metadata": {
        "id": "3c6MxojNCeqF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Mount Google Drive**"
      ],
      "metadata": {
        "id": "H4rfZWB1CuzK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Rrz_5Y5AM_v"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####**Import Necessary Library**"
      ],
      "metadata": {
        "id": "M4KXXe-BC1EY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "import glob as gb\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "kQMrCRzPC4V5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
        "from tensorflow.keras.layers.experimental.preprocessing import RandomFlip, RandomRotation\n",
        "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
        "from tensorflow.keras.layers import Conv2D, MaxPool2D ,LeakyReLU"
      ],
      "metadata": {
        "id": "06Mq3xHyHBjZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####**Define directories path**"
      ],
      "metadata": {
        "id": "ltxTsgMlr1q_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Define the directories for training, testing, and validation\n",
        "train_directory = '/content/drive/MyDrive/archive/Test_Train_Val/train'\n",
        "test_directory = '/content/drive/MyDrive/archive/Test_Train_Val/test'\n",
        "valid_directory = '/content/drive/MyDrive/archive/Test_Train_Val/val'"
      ],
      "metadata": {
        "id": "j5iYJBbXLam1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####**Create dataset function using keras/Tensorflow**"
      ],
      "metadata": {
        "id": "99Pc8KxS6L6V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "IMG_SIZE = (299, 299)  # define resolution (299,299) /(224,224)\n",
        "BATCH_SIZE = 128       # varies from dataset to datset prefferable 128/68/32"
      ],
      "metadata": {
        "id": "bcSESb1NFk0x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create TensorFlow datasets for training, testing, and validation\n",
        "#you can customize parameters as per dataset\n",
        "train_dataset = image_dataset_from_directory(\n",
        "    train_directory,\n",
        "    shuffle=True,\n",
        "    labels='inferred',\n",
        "    batch_size=BATCH_SIZE,\n",
        "    image_size=IMG_SIZE,\n",
        "    color_mode='rgb',\n",
        "    seed=42\n",
        ")\n",
        "\n",
        "test_dataset = image_dataset_from_directory(\n",
        "    test_directory,\n",
        "    shuffle=True,\n",
        "    labels='inferred',\n",
        "    batch_size=BATCH_SIZE,\n",
        "    image_size=IMG_SIZE,\n",
        "    color_mode='rgb',\n",
        "    seed=42\n",
        ")\n",
        "\n",
        "valid_dataset = image_dataset_from_directory(\n",
        "    valid_directory,\n",
        "    shuffle=True,\n",
        "    labels='inferred',\n",
        "    batch_size=BATCH_SIZE,\n",
        "    image_size=IMG_SIZE,\n",
        "    color_mode='rgb',\n",
        "    seed=42\n",
        ")"
      ],
      "metadata": {
        "id": "6Q5mCJtiF5_p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Optional: You can also specify the class names if you have a specific order for your classes\n",
        "class_names = train_dataset.class_names\n",
        "\n",
        "# Print class names\n",
        "print(\"Class Names:\", class_names)"
      ],
      "metadata": {
        "id": "QwhgT6uBGVP4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#plot bar chart for demonstrating data size\n",
        "sns.set_theme(style=\"whitegrid\")\n",
        "Dataset = []\n",
        "for folder in os.listdir(train_directory):\n",
        "    files = gb.glob(pathname=str(train_directory + \"/\" + folder +\"/*.*\"))\n",
        "    Dataset.append(len(files))\n",
        "plt.figure(figsize=(13,7))                    ## change the len size\n",
        "sns.barplot(x=[ \"0\",\"1\",\"2\",\"3\",\"4\"], y=Dataset, palette=\"rocket\")  ## change the class name\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ij6ZO6QQGa0B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <font color='blue'>Build the Model</font>"
      ],
      "metadata": {
        "id": "n_GMjBaXOpD6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "base_learning_rate = 0.001 # prefferable lr is 0.0001 or 0.001\n",
        "IMG_SHAPE = IMG_SIZE +(3,)"
      ],
      "metadata": {
        "id": "-OalbxVyBbHA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## N.B:download the weights (imagenet/...) and model (Resnet50/ResNet100/InceptionV3...)\n",
        "base_model = tf.keras.applications.ResNet50(input_shape=IMG_SHAPE, include_top= False, weights='imagenet')"
      ],
      "metadata": {
        "id": "tl9Pm1lQO6Up"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(base_model)\n",
        "## N.B: changle the model name\n",
        "preprocess_input = tf.keras.applications.resnet.preprocess_input"
      ],
      "metadata": {
        "id": "wJWvn2iEO92h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nb_layers = len(base_model.layers)\n",
        "print(\"Numbers of Layers =\" , nb_layers)\n",
        "print(base_model.layers[nb_layers - 2].name)  # pre- Last name\n",
        "print(base_model.layers[nb_layers - 1].name)"
      ],
      "metadata": {
        "id": "hHByKSYiPAfk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# iterate over first batch (32 image) in trainset\n",
        "image_batch, label_batch = next(iter(train_dataset))  # 32 image arrays\n",
        "feature_batch = base_model(image_batch)  # run the model on those 32 image (base model with its 1000 causes classification)\n",
        "print(feature_batch.shape)  # 32 for number of images in this batch and 1000 for classes"
      ],
      "metadata": {
        "id": "02wLshtjPBxI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_model( image_shape=IMG_SHAPE):\n",
        "    ''' Define a tf.keras model for multi-class classification out of the *model name* (Resnet/Inception...) '''\n",
        "    ##change the *model name*\n",
        "    downloaded_model = tf.keras.applications.ResNet50(input_shape=IMG_SHAPE, include_top= False, weights='imagenet')\n",
        "    downloaded_model.trainable = True\n",
        "    for layer in downloaded_model.layers[0 : 291]:\n",
        "        layer.trainable = False\n",
        "\n",
        "    inputs = tf.keras.Input(image_shape)\n",
        "    x = preprocess_input(inputs)\n",
        "    x = downloaded_model(x , training=False)\n",
        "    x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
        "    x = tf.keras.layers.Dropout(0.5)(x)\n",
        "    prediction_layer = tf.keras.layers.Dense(7 ,activation = \"softmax\")   ## change the first parameter according to the class len\n",
        "    outputs = prediction_layer(x)\n",
        "    model = tf.keras.Model(inputs, outputs)\n",
        "    model.summary()\n",
        "    return model"
      ],
      "metadata": {
        "id": "oIUFhaRnPgYj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## specify function name as model name\n",
        "model = create_model(IMG_SHAPE)"
      ],
      "metadata": {
        "id": "IA6HsX4xY_k1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## customize optimizer as Nadam or Adam\n",
        "model.compile(optimizer=tf.keras.optimizers.Nadam(learning_rate=base_learning_rate),\n",
        "                           loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
        "                           metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "5snFX_C-cbRe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "## set the path name as *dataset/Lr/optimizer_name/model_name*\n",
        "model_filepath=\"/content/drive/MyDrive/dataset/Lr/optimizer_name/model_name-{epoch:02d}-{val_accuracy:.4f}.hdf5\"\n",
        "checkpoint = ModelCheckpoint(\n",
        "    filepath = model_filepath ,\n",
        "    monitor ='val_accuracy',\n",
        "    mode = 'max' ,\n",
        "    save_best_only =True ,\n",
        "    verbose = 1\n",
        ")"
      ],
      "metadata": {
        "id": "EebFVeeCc6et"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#avoid random weight initialization do automatic if you can\n",
        "\n",
        "total = 0\n",
        "for i in range(0,len(Dataset )) :\n",
        "    total +=Dataset [i]\n",
        "\n",
        "weight_for_0 = (1 / Dataset [0]) * (total / 5.0)\n",
        "weight_for_1 = (1 / Dataset [1]) * (total / 5.0)\n",
        "weight_for_2 = (1 / Dataset [2]) * (total / 5.0)\n",
        "weight_for_3 = (1 / Dataset [3]) * (total / 5.0)\n",
        "weight_for_4 = (1 / Dataset [4]) * (total / 5.0)\n",
        "\n",
        "\n",
        "class_weight = {0: weight_for_0, 1: weight_for_1, 2: weight_for_2, 3: weight_for_3,4: weight_for_4}\n",
        "\n",
        "print('Weight for class 0: {:.2f}'.format(weight_for_0))\n",
        "print('Weight for class 1: {:.2f}'.format(weight_for_1))\n",
        "print('Weight for class 2: {:.2f}'.format(weight_for_2))\n",
        "print('Weight for class 3: {:.2f}'.format(weight_for_3))\n",
        "print('Weight for class 4: {:.2f}'.format(weight_for_4))"
      ],
      "metadata": {
        "id": "MXhh4s7xDk3x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####**Training model**"
      ],
      "metadata": {
        "id": "MQ70xk06BAhR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## change hyperparameter such as epoches\n",
        "history = model.fit(train_dataset , verbose=2 , epochs=50 , class_weight=class_weight ,\n",
        "                               validation_data=valid_dataset , use_multiprocessing= True, callbacks =[checkpoint])"
      ],
      "metadata": {
        "id": "ASDsHJqQdXk2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.evaluate(test_dataset , verbose = 1)"
      ],
      "metadata": {
        "id": "iDopjvLRdc0_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Model evaluating on Test dataset**"
      ],
      "metadata": {
        "id": "3v7siWwLvSG3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(20, 15))\n",
        "for images, labels in test_dataset.take(1):\n",
        "    for i in range(25):\n",
        "        plt.subplot(5,5,i+1)\n",
        "        plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
        "        plt.axis(\"off\")\n",
        "        im2 = images[i].numpy().astype(\"uint8\")\n",
        "        img2 = tf.expand_dims(im2, 0)\n",
        "        predict = model.predict(img2)\n",
        "        predicted= class_names[np.argmax(predict)]\n",
        "        actual = class_names [labels[i].numpy().astype(\"uint8\")]\n",
        "        if (actual == predicted):\n",
        "            plt.title(predicted, fontsize=10, color= 'blue', pad=15);\n",
        "        else :\n",
        "            plt.title(actual, fontsize=10, color= 'red' ,pad=15);\n",
        "        plt.subplots_adjust(left=0.1,bottom=0.1, right=0.9,\n",
        "                            top=0.9, wspace=0.4,hspace=0.4)"
      ],
      "metadata": {
        "id": "Q6tp2J0udexP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**<font color='green'>Training and Validation Accuracy </font> & <font color='red'>Training and Validation Loss </font>**"
      ],
      "metadata": {
        "id": "eiTyZRgCpx2b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "acc = [0.] + history.history['accuracy']\n",
        "val_acc = [0.] + history.history['val_accuracy']\n",
        "\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "plt.figure(figsize=(17, 12))\n",
        "plt.subplot(2, 2, 1)\n",
        "plt.plot(acc, label='Training Accuracy')\n",
        "plt.plot(val_acc, label='Validation Accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.ylim([min(plt.ylim()),1])\n",
        "plt.title('Training and Validation Accuracy')\n",
        "\n",
        "plt.subplot(2, 2, 2)\n",
        "plt.plot(loss, label='Training Loss')\n",
        "plt.plot(val_loss, label='Validation Loss')\n",
        "plt.legend(loc='upper right')\n",
        "plt.ylabel('Cross Entropy')\n",
        "plt.ylim([0,3.0])\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "cffTY7xIdglH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**True and Predicted labels**"
      ],
      "metadata": {
        "id": "-IVeTL6Ks5Gm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import label_binarize\n",
        "\n",
        "# Initialize empty lists to store true labels and predicted labels\n",
        "true_labels = []\n",
        "predicted_labels = []\n",
        "\n",
        "# Iterate through the validation dataset and make predictions\n",
        "for images, labels in test_dataset:\n",
        "    predictions = model.predict(images)\n",
        "    predicted_labels.extend(np.argmax(predictions, axis=1))\n",
        "    true_labels.extend(labels.numpy())\n",
        "\n",
        "# Binarize the true and predicted labels\n",
        "true_labels_bin = label_binarize(true_labels, classes=np.unique(true_labels))\n",
        "predicted_labels_bin = label_binarize(predicted_labels, classes=np.unique(predicted_labels))"
      ],
      "metadata": {
        "id": "e9WMIn6qspog"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Classification Report**"
      ],
      "metadata": {
        "id": "TtyzzXJ4qarM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Generate the classification report\n",
        "report = classification_report(true_labels, predicted_labels, target_names=class_names)\n",
        "\n",
        "# Print the classification report\n",
        "print(report)"
      ],
      "metadata": {
        "id": "dS3ZX7Srqa4n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**ROC Curve for Multiclass Classification**"
      ],
      "metadata": {
        "id": "ybdZaU84rNYz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_auc_score, roc_curve, auc\n",
        "\n",
        "# Compute class-specific ROC AUC values\n",
        "roc_auc_per_class = []\n",
        "for i in range(len(class_names)):\n",
        "    roc_auc = roc_auc_score(true_labels_bin[:, i], predicted_labels_bin[:, i])\n",
        "    roc_auc_per_class.append(roc_auc)\n",
        "\n",
        "# Plot class-specific ROC curves\n",
        "plt.figure(figsize=(10, 8))\n",
        "\n",
        "for i in range(len(class_names)):\n",
        "    fpr, tpr, _ = roc_curve(true_labels_bin[:, i], predicted_labels_bin[:, i])\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "\n",
        "    plt.plot(fpr, tpr, label=f'ROC curve (area = {roc_auc:.2f}) for {class_names[i]}')\n",
        "\n",
        "plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "##change the model name\n",
        "plt.title('ROC Curves for Multiclass Classification of *model name*')\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()\n",
        "\n",
        "# Print class-specific ROC AUC values\n",
        "for i in range(len(class_names)):\n",
        "    print(f'ROC AUC for {class_names[i]}: {roc_auc_per_class[i]:.4f}')"
      ],
      "metadata": {
        "id": "81NLPeZSrNkw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Confusion Matrix**"
      ],
      "metadata": {
        "id": "DM_N8OgCpAGZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "conf_matrix = confusion_matrix(true_labels, predicted_labels)\n",
        "\n",
        "# Plot the confusion matrix as a heatmap\n",
        "plt.figure(figsize=(5, 3))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
        "plt.xlabel('Predicted Labels')\n",
        "plt.ylabel('True Labels')\n",
        "## change the model name\n",
        "plt.title('Confusion Matrix of *model_name*')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "J9gWVFubdk0Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Cohen's Kappa**"
      ],
      "metadata": {
        "id": "7akuXLlkpT3J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import cohen_kappa_score\n",
        "\n",
        "kappa = cohen_kappa_score(true_labels, predicted_labels)\n",
        "print(f'Cohen\\'s Kappa: {kappa:.4f}')"
      ],
      "metadata": {
        "id": "8ggV91WzpUCl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Matthews Correlation Coefficient (MCC)**"
      ],
      "metadata": {
        "id": "IABgqZUqpUUe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import matthews_corrcoef\n",
        "import numpy as np\n",
        "\n",
        "# Assuming true_labels and predicted_labels are multiclass labels\n",
        "mcc_values = [matthews_corrcoef(true_labels == i, predicted_labels == i) for i in np.unique(true_labels)]\n",
        "\n",
        "average_mcc = np.mean(mcc_values)\n",
        "print(f'Average Matthews Correlation Coefficient for Multiclass: {average_mcc:.4f}')"
      ],
      "metadata": {
        "id": "ujJsadsRpUq9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}